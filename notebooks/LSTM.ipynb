{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: try SHAP/permutation importance on a simpler model first\n",
    "# (e.g., LightGBM) to reduce noise, then feed selected features to LSTM.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "from keras import layers, callbacks, models, Sequential, Input\n",
    "\n",
    "# ---------------------\n",
    "# Configurable parameters\n",
    "# ---------------------\n",
    "#CSV_PATH   = \"new_data.csv\"      # path to your uploaded file\n",
    "TARGET_COL = \"GC_F_Close\"        # predict tomorrow's close for gold futures\n",
    "#DATE_COL   = \"Date\"\n",
    "LOOKBACK   = 30               # number of past days in each sequence\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 100\n",
    "VAL_PATIENCE = 8                 # early stopping patience\n",
    "LR_REDUCE_PATIENCE = 4\n",
    "LAGS = [1, 5, 10, 20]\n",
    "\n",
    "\"\"\"# ---------------------\n",
    "# 1) Load & basic cleaning\n",
    "# ---------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Ensure datetime and sort chronologically # pourquoi if, try ?\n",
    "if DATE_COL in df.columns:\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "# Drop rows where target is missing\n",
    "df = df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "# Create tomorrow's target by shifting -1 (next day)\n",
    "df[\"target_tomorrow\"] = df[TARGET_COL].shift(-1)\n",
    "\n",
    "# After shift, last row has NaN target; drop it\n",
    "df = df.dropna(subset=[\"target_tomorrow\"]).reset_index(drop=True)\n",
    "\n",
    "# Lag features: add past values of the target as exogenous inputs # pas compris cette partie\n",
    "#for lag in LAGS:\n",
    "    #df[f\"{TARGET_COL}_lag{lag}\"] = df[TARGET_COL].shift(lag)\"\"\"\n",
    "\n",
    "# ---------------------\n",
    "# 2) Feature selection & imputation\n",
    "# ---------------------\n",
    "# Inputs = all columns except DATE_COL and TARGET_COL; exclude helper columns too\n",
    "exclude = {\"GC_F_Close\"}\n",
    "feature_cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "# Time-series friendly imputation: forward-fill then back-fill\n",
    "X_raw = df[feature_cols]\n",
    "\n",
    "#.copy().ffill().bfill()\n",
    "\n",
    "# Target vector\n",
    "y_raw = df[\"GC_F_Close\"].values.astype(np.float32)\n",
    "\n",
    "# ---------------------\n",
    "# 3) Chronological split: train / val / test\n",
    "# ---------------------\n",
    "n = len(df)\n",
    "train_end = int(n * 0.70)\n",
    "val_end   = int(n * 0.85)\n",
    "\n",
    "X_train_raw = X_raw.iloc[:train_end].copy()\n",
    "X_val_raw   = X_raw.iloc[train_end:val_end].copy()\n",
    "X_test_raw  = X_raw.iloc[val_end:].copy()\n",
    "\n",
    "y_train = y_raw[:train_end]\n",
    "y_val   = y_raw[train_end:val_end]\n",
    "y_test  = y_raw[val_end:]\n",
    "\n",
    "# ---------------------\n",
    "# 4) Scale features (fit on train only)\n",
    "# ---------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled   = scaler.transform(X_val_raw)\n",
    "X_test_scaled  = scaler.transform(X_test_raw)\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# ---------------------\n",
    "# 5) Build rolling windows (LSTM sequences)\n",
    "# ---------------------\n",
    "\n",
    "def make_sequences(X, y, lookback):\n",
    "    #Build sequences of shape (num_samples, lookback, num_features) and aligned targets.\n",
    "    #Each sequence uses X[t-lookback:t] to predict y[t].\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for t in range(lookback, len(X)):\n",
    "        X_seq.append(X[t - lookback:t])\n",
    "        y_seq.append(y[t])\n",
    "    return np.asarray(X_seq, dtype=np.float32), np.asarray(y_seq, dtype=np.float32)\n",
    "\n",
    "X_tr_seq, y_tr_seq = make_sequences(X_train_scaled, y_train, LOOKBACK)\n",
    "X_va_seq, y_va_seq = make_sequences(X_val_scaled,   y_val,   LOOKBACK)\n",
    "X_te_seq, y_te_seq = make_sequences(X_test_scaled,  y_test,  LOOKBACK)\n",
    "\n",
    "print(f\"Train seq shape: {X_tr_seq.shape}, Val: {X_va_seq.shape}, Test: {X_te_seq.shape}\")\n",
    "\n",
    "# ---------------------\n",
    "# 6) Define the LSTM model\n",
    "# ---------------------\n",
    "def build_model(n_features, lookback):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(lookback, n_features)))\n",
    "\n",
    "    # LSTM principal\n",
    "    model.add(layers.LSTM(512, return_sequences=True))\n",
    "    model.add(layers.LSTM(units=256,return_sequences=False))\n",
    "\n",
    "    # Dense non lin√©aire\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    # Sortie\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(n_features,LOOKBACK)\n",
    "model.summary()\n",
    "\n",
    "# ---------------------\n",
    "# 7) Callbacks\n",
    "# ---------------------\n",
    "ckpt_path = \"best_lstm_gold.keras\"\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\", patience=VAL_PATIENCE, restore_best_weights=True, verbose=1),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=LR_REDUCE_PATIENCE, factor=0.5, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "\n",
    "# ---------------------\n",
    "# 8) Train\n",
    "# ---------------------\n",
    "history = model.fit(\n",
    "    X_tr_seq, y_tr_seq,\n",
    "    validation_data=(X_va_seq, y_va_seq),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,        # important for time series\n",
    "    callbacks=cbs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------\n",
    "# 9) Evaluate on test\n",
    "# ---------------------\n",
    "y_pred_test = model.predict(X_te_seq).squeeze()\n",
    "\n",
    "mae  = mean_absolute_error(y_te_seq, y_pred_test)\n",
    "rmse = sqrt(mean_squared_error(y_te_seq, y_pred_test))\n",
    "\n",
    "def mape(a, f):\n",
    "    return np.mean(np.abs((a - f) / np.clip(np.abs(a), 1e-8, None))) * 100\n",
    "\n",
    "mape_val = mape(y_te_seq, y_pred_test)\n",
    "\n",
    "print(f\"Test MAE:  {mae:,.4f}\")\n",
    "print(f\"Test RMSE: {rmse:,.4f}\")\n",
    "print(f\"Test MAPE: {mape_val:,.2f}%\")\n",
    "\n",
    "# ---------------------\n",
    "# 10) Plot predictions vs actual\n",
    "# ---------------------\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_te_seq, label=\"Actual (tomorrow GC=F_Close)\")\n",
    "plt.plot(y_pred_test, label=\"Predicted\", alpha=0.8)\n",
    "plt.title(\"LSTM prediction: tomorrow GC=F_Close\")\n",
    "plt.xlabel(\"Test time steps\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lstm_gold_test_plot.png\", dpi=140)\n",
    "\n",
    "# ---------------------\n",
    "# 11) Save predictions and scaler\n",
    "# ---------------------\n",
    "pd.DataFrame({\n",
    "    \"y_true\": y_te_seq,\n",
    "    \"y_pred\": y_pred_test\n",
    "}).to_csv(\"lstm_gold_test_predictions.csv\", index=False)\n",
    "\n",
    "# Save scaler for inference consistency\n",
    "import joblib\n",
    "joblib.dump(scaler, \"feature_scaler.pkl\")\n",
    "\n",
    "print(\"Artifacts saved: best_lstm_gold.keras, lstm_gold_test_plot.png, lstm_gold_test_predictions.csv, feature_scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
